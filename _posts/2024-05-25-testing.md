---
layout: post
title: "Testing Tenets"
description: "Speed is important"
---

Some thoughts on testing I've been mulling over.

### Style: No nesting

Makes search easier, with nesting you'll have tests named `it "success" do`, and there will be bunch of them.

### Style: No before each

`before_each` and friends [introduce globals](https://kentcdodds.com/blog/avoid-nesting-when-youre-testing) -- they're often nullable so you have `!` (or equivalent) sprinkled everywhere.

### Style: Use factory functions for test data

This is mostly calling out [pytest fixtures](https://docs.pytest.org/en/8.2.x/explanation/fixtures.html).

While JetBrains' IDEs and VSCode (via Pylance) support navigating to the fixture definitions from their usages, fixtures in a general are globals that are hard to maintain as projects get larger.

[Much easier](https://www.youtube.com/watch?v=ickNQcNXiS4&t=985s) to [have factory functions](https://lukeplant.me.uk/blog/posts/test-factory-functions-in-django/), e.g., `make_user(name: str) -> User`. But they can [also have issues](https://brandur.org/t-parallel#deadlocks).

### Style: suffix test files with `_test.`, `.test.`, `.spec.` instead of prefixing them

I like that [`foo_test.py`](https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#conventions-for-python-test-discovery) sorts next to `foo.py`, `test_foo.py` doesn't.

### Style: Tests exist next to the code they're testing

Different ecosystems have different approaches around this, but having the tests close to the code they're testing makes it easier to glance coverage.

### DX: Editor Support

Running a one-off test via your editor of choice should be easy.

Debugging and assertion diffs should also be easy.

Vitest has [a good VSCode integration](https://github.com/vitest-dev/vscode).

### DX: Watch mode

There should be an option to run tests in watch mode and rerun on file changes, [Vitest does this well](https://vitest.dev/guide/features#watch-mode).

### Correctness: No order dependencies

Ensures you can run a one off test and believe its results.

Also makes sharding at the test level easier.

See [`i_suck_and_my_tests_are_order_dependent!`](https://www.rubydoc.info/gems/minitest/Minitest%2FTest.i_suck_and_my_tests_are_order_dependent!) and [related discussion](https://news.ycombinator.com/item?id=34826813).

### Correctness: Tests actually commit transactions

If you're project involves database transactions you'll want to actually `commit` them in your tests. Django doesn't do this by default, and it bites you when you have more complicated queries and setups. This also means you shouldn't mock your database.

### Correctness: Easy way to mock common components

If you're tests use S3, SQS, etc. there should be an easy and robust way to mock them.

### Perf: Donâ€™t cleanup after each test

[Dropping the database, truncating tables](https://conroy.org/per-test-database-isolation-in-postgres), clearing directories, etc. are too slow to run after each test.

Let the test data stick around, tests should be robust enough to not require a clean slate.

### Perf: All tests run in parallel

This requires tests to be okay with other data existing in the database, which seems like a fair trade-off, production will have a bunch of preexisting data anyways.

Flakiness might be reason to avoid running everything in parallel but that's an issue that can be handled on its own.

Also the test runner should take advantage of multiple cores!

### Perf: Sharding

Shard [at the test level](https://github.com/microsoft/playwright/issues/21497#issuecomment-1472355750), not file/module.

Sharding also enables [spreading tests across VMs in CI](https://www.edgedb.com/blog/how-we-sharded-our-test-suite-for-10x-faster-runs-on-github-actions).

### Perf: Asyncio support

Somewhat Python specific, but async support should be builtin.

### Perf: Purity

By default any Sentry errors, warnings, [unmocked network requests](https://laravel.com/docs/9.x/http-client#preventing-stray-requests) generated in the test should fail the test.

### Perf: Limits

Each test should have a time limit, which should be adjustable on a one off basis.

### Perf: Flakiness detection and quarantining

Requires integration into CI, but test flakiness should be automatically detected and the tests should be disabled until they're fixed.

### Perf: Easy performance info

It [should be](https://www.honeycomb.io/blog/why-are-tests-slow) easy to [find slow tests](https://blog.sentry.io/instrumenting-our-frontend-test-suite-and-fixing-what-we-found/).

### Perf: Speed

Tests [should be fast](https://brandur.org/nanoglyphs/029-path-of-madness) to run.
